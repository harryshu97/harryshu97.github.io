---
layout: default
title: PaperList
---

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Academic Papers Collection</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 20px;
            background-color: #f4f4f4;
            color: #B6390A;
        }
        h1, h2 {
            color: #B6390A;
        }
        p {
            margin: 10px 0;
        }
        strong {
            font-weight: bold;
            color: #0AA4B6;
        }
    </style>
</head>
<body>
    <h1>ML System Papers</h1>
    <p>Boosting DNN Cold Inference on Edge Devices</p>
    <p><strong>STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining</strong></p>
    <p>Data movement is all you need</p>
    <p><strong>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</strong></p>
    <p>Analytical Cache Modeling and Tilesize Optimization for Tensor Contractions</p>
    <p><strong>CHECKMATE: BREAKING THE MEMORY WALL WITH OPTIMAL TENSOR REMATERIALIZATION</strong></p>
    <p>Open Compute Project • OCP Microscaling Formats (MX) Specification</p>
    <p>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</p>
    <p>Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators</p>
    <p><strong>Ansor: Generating High-Performance Tensor Programs for Deep Learning</strong></p>
    <p>PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</p>
    <p><strong>LLM in a flash: Efficient Large Language Model Inference with Limited Memory</strong></p>
    <p>MOPAR: A Model Partitioning Framework for Deep Learning Inference Services on Serverless Platforms</p>
    <p>LLMCad: Fast and Scalable On-device Large Language Model Inference</p>
    <p>PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML</p>
    <p>Fairness in Serving Large Language Models</p>
    <p>DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</p>
    <p>Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</p>
    <p><strong>PipeEdge: Pipeline Parallelism for Large-Scale Model Inference on Heterogeneous Edge Devices</strong></p>
    <p>MAGIS: Memory Optimization via Coordinated Graph Transformation and Scheduling for DNN</p>
    <p><strong>MGG: Accelerating Graph Neural Networks with Fine-Grained Intra-Kernel Communication-Computation Pipelining on Multi-GPU Platforms</strong></p>
    <p>ROLLER: Fast and Efficient Tensor Compilation for Deep Learning</p>
        <p>MAGIS: Memory Optimization via Coordinated Graph Transformation and Scheduling for DNN</p>
    <p>Optimizing DNN Compilation for Distributed Training With Joint OP and Tensor Fusion</p>
    <p>Practical Edge Kernels for Integer-Only Vision Transformers Under Post-training Quantization</p>
    <p>PyTorch: An Imperative Style, High-Performance Deep Learning Library</p>
    <p><strong>PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation</strong></p>
    <p><strong>RAMMER: Enabling Holistic Deep Learning Compiler Optimizations with rTasks</strong></p>
    <p><strong>Romou: rapidly generate high-performance tensor kernels for mobile GPUs</strong></p>
    <p><strong>SmartMem: Layout Transformation Elimination and Adaptation for Efficient DNN Execution on Mobile</strong></p>
    <p>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</p>
    <p>Sparse GPU Kernels for Deep Learning</p>
    <p><strong>Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations</strong></p>
    <p><strong>STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining</strong></p>
    <p>Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code</p>
    <p>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</p>
    <p>Automatic generation of high-performance quantized machine learning kernels</p>
    <p>Data Movement Is All You Need: A Case Study on Optimizing Transformers</p>
    <p><strong>POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging</strong></p>
    <p>PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</p>
    <p><strong>Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning</strong></p>

    <h2>LLM</h2>
    <p>DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</p>
    <p>Fairness in Serving Large Language Models</p>
    <p>FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU</p>
    <p>LLM in a flash: Efficient Large Language Model Inference with Limited Memory</p>
    <p>LLM-FP4: 4-Bit Floating-Point Quantized Transformers</p>
    <p>PointLLM: Empowering Large Language Models to Understand Point Clouds</p>
    <p>Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</p>

    <h2>Nerf</h2>
    <p>TripoSR: Fast 3D Object Reconstruction from a Single Image</p>
    <p>VR-NeRF: High-Fidelity Virtualized Walkable Spaces</p>

    <h2>Pruning</h2>
    <p><strong>PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning</strong></p>
    <p>RT3D: Achieving Real-Time Execution of 3D Convolutional Neural Networks on Mobile Devices</p>
    <p>YOLObile: Real-Time Object Detection on Mobile Devices via Compression-Compilation Co-Design</p>

    <h2>Pipeline</h2>
    <p>Software pipelining: an effective scheduling technique for VLIW machines</p>

    <h2>Attention Optimization</h2>
    <p>Full Stack Optimization of Transformer Inference: a Survey</p>
    <p>HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers</p>
    <p>MixFormerV2: Efficient Fully Transformer Tracking</p>
    <p><strong>Token Merging: Your ViT But Faster</strong></p>
    <p>ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design</p>

    <h2>Cache Optimization</h2>
    <p>Analytical cache modeling and tilesize optimization for tensor contractions</p>
    <p>Enabling Large Dynamic Neural Network Training with Learning-based Memory Management on Tiered Memory</p>
    <p><strong>MOPAR: A Model Partitioning Framework for Deep Learning Inference Services on Serverless Platforms</strong></p>
    <p>Understanding Cache Boundness of ML Operators on ARM Processors</p>

    <h2>Compiler</h2>
    <p>MLIR: A Compiler Infrastructure for the End of Moore's Law</p>
    <p>MNN: A Universal and Efficient Inference Engine</p>
    <p><strong>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</strong></p>

    <h2>Distributed System</h2>
    <p>MapReduce: a flexible data processing tool</p>

    <h2>DNN Optimization</h2>
    <p>AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures</p>
    <p><strong>Boosting DNN Cold Inference on Edge Devices</strong></p>
    <p>Data Layout Transformation for Stencil Computations on Short-Vector SIMD Architectures</p>
    <p>DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion</p>

        <p>Dynamic Network Surgery for Efficient DNNs</p>
    <p>Fast Algorithms for Convolutional Neural Networks</p>
    <p>GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</p>
    <p>InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning</p>
    <p>Kodan: Addressing the Computational Bottleneck in Space</p>
    <p>MixFormerV2: Efficient Fully Transformer Tracking</p>
    <p>PCONV: The Missing but Desirable Sparsity in DNN Weight Pruning for Real-time Execution on Mobile Devices</p>
    <p>ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design</p>
    <p>Efficiently Scaling Transformer Inference</p>
    <p>Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication</p>

    <h2>Algorithm</h2>
    <p>Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators</p>
    <p>Faster Matrix Multiplication via Asymmetric Hashing</p>

    <h2>Official Docs</h2>
    <p>Arm® Cortex®‑X3 Core Technical Reference Manual</p>
    <p>Qualcomm Snapdragon Mobile Platform OpenCL General Programming and Optimization</p>

    <h2>Diffusion Models</h2>
    <p>A Survey on Video Diffusion Models</p>
    <p><strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong></p>
    <p><strong>Consistency Models</strong></p>
    <p>Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</p>
    <p>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</p>
    <p>Make Pixels Dance: High-Dynamic Video Generation</p>
    <p>DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation</p>
    <p><strong>DeepCache: Accelerating Diffusion Models for Free</strong></p>
    <p>Denoising Diffusion Implicit Models</p>
    <p>Denoising Diffusion Probabilistic Models</p>
    <p>DiffiT: Diffusion Vision Transformers for Image Generation</p>
    <p>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</p>
    <p>GAIA: Zero-shot Talking Avatar Generation</p>
    <p>High-Resolution Image Synthesis with Latent Diffusion Models</p>
    <p>InstantID: Zero-shot Identity-Preserving Generation in Seconds</p>
    <p>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</p>
    <p>MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices</p>
    <p>Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models</p>
    <p>Pseudo Numerical Methods for Diffusion Models on Manifolds</p>
    <p>Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</p>
    <p>Structural Pruning for Diffusion Models</p>
    <p>Token Merging for Fast Stable Diffusion</p>
    <p><strong>Tutorial on Diffusion Models for Imaging and Vision</strong></p>
    <p>UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs</p>

</body>
</html>
